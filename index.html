<!doctype html>
<html lang="ar">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
<title>Jaffar — Live Side Detector (Enhanced)</title>
<link rel="icon" href="data:,">
<style>
  :root{
    --bg:#071021; --panel:rgba(255,255,255,0.04);
    --accent:#1e90ff; --muted:#9fb6c9; --glass:rgba(255,255,255,0.03);
  }
  *{box-sizing:border-box}
  html,body{height:100%;margin:0;font-family:Inter,system-ui,-apple-system,Segoe UI,Roboto,Arial;background:var(--bg);color:#eaf4fb}
  .container{max-width:1100px;margin:18px auto;padding:18px}
  header{display:flex;gap:12px;align-items:center;justify-content:space-between}
  h1{font-size:20px;margin:0;color:#dff4ff}
  p.lead{margin:4px 0 0 0;color:var(--muted);font-size:13px}
  .controls{display:flex;gap:8px;align-items:center;flex-wrap:wrap;margin-top:12px}
  button,select,input[type=range]{background:var(--accent);border:none;color:#012;padding:9px 12px;border-radius:10px;font-weight:700;cursor:pointer}
  button.secondary{background:#2d3b4a;color:#eaf4fb}
  .panel{background:var(--panel);padding:14px;border-radius:14px;border:1px solid rgba(255,255,255,0.04);backdrop-filter:blur(8px);margin-top:14px}
  .layout{display:flex;gap:14px;flex-wrap:wrap}
  .left{flex:1 1 600px;min-width:300px}
  .right{flex:0 1 340px;min-width:260px}
  .stack{position:relative;border-radius:12px;overflow:hidden;background:#0b0f14;border:1px solid rgba(255,255,255,0.02)}
  video#cam{display:block;width:100%;height:auto;background:#000}
  canvas.overlay{position:absolute;left:0;top:0;pointer-events:none}
  .status{margin-top:12px;padding:10px;border-radius:10px;background:linear-gradient(180deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01));color:var(--muted);font-weight:700}
  .small{font-size:13px;color:var(--muted)}
  .controls .conf{display:flex;gap:8px;align-items:center;margin-left:auto}
  .pill{padding:6px 10px;border-radius:10px;background:var(--glass);border:1px solid rgba(255,255,255,0.03);color:#cfefff;font-weight:700}
  .btn-row{display:flex;gap:8px;flex-wrap:wrap;margin-top:8px}
  footer{margin-top:18px;text-align:center;color:var(--muted);font-size:13px}
  .pretty{padding:10px;border-radius:10px;background:linear-gradient(180deg, rgba(255,255,255,0.01), rgba(255,255,255,0.02));border:1px solid rgba(255,255,255,0.02)}
  .switch{display:inline-flex;align-items:center;gap:8px}
  .hint{color:#ffd36b;margin-top:10px}
  @media(max-width:820px){ .layout{flex-direction:column} .controls .conf{margin-left:0} }
</style>
</head>
<body>
  <div class="container">
    <header>
      <div>
        <h1>Jaffar ∞ Live Side Detector — Enhanced</h1>
        <p class="lead">Phone = car. Uses TF.js coco-ssd with multiple improvements: backend pick, warmup, side-cropping, smoothing, HUD & prettier UI.</p>
      </div>
      <div class="pill">Version: enhanced</div>
    </header>

    <div class="controls">
      <button id="startBtn">Start (اطلب الكاميرا)</button>
      <button id="stopBtn" class="secondary">Stop</button>

      <label class="switch"><input id="testMode" type="checkbox" checked> Test mode (use reference image)</label>

      <select id="camSelect" title="Camera" style="padding:8px;border-radius:10px;background:#08121a;color:#cde3ff;border:1px solid rgba(255,255,255,0.03)"></select>

      <div class="conf">
        <label class="small">Confidence</label>
        <input id="conf" type="range" min="0.2" max="0.95" step="0.05" value="0.55">
        <span id="confVal" class="pill">0.55</span>
      </div>
    </div>

    <div class="panel layout">
      <div class="left">
        <div class="stack" id="stack" style="aspect-ratio:16/9;">
          <video id="cam" autoplay playsinline muted></video>
          <canvas id="boxes" class="overlay"></canvas>
          <canvas id="hud" class="overlay"></canvas>
        </div>

        <div class="status pretty" id="desc">No side objects detected.</div>
        <div class="hint" id="hint" style="display:none"></div>
      </div>

      <div class="right">
        <div class="pretty">
          <div style="font-weight:800;margin-bottom:6px">Debug / Tools</div>
          <div class="btn-row">
            <button id="showLeft" class="secondary">Show Left</button>
            <button id="showRight" class="secondary">Show Right</button>
            <button id="showBoth" class="secondary">Show Both</button>
            <button id="clear" class="secondary">Clear</button>
          </div>

          <div style="margin-top:12px;font-size:13px;color:var(--muted)">
            <div><b>Test image (embedded):</b></div>
            <div style="font-family:monospace;margin-top:6px;color:#9fb6c9" id="imgPath">/mnt/data/E99A7BF9-F4D9-4F41-9C8A-0751B5C9F4E2.jpeg</div>
            <div style="margin-top:8px">Open via HTTPS or <code>localhost</code> for live camera access.</div>
          </div>
        </div>

        <div class="pretty" style="margin-top:12px">
          <div style="font-weight:800;margin-bottom:6px">Settings</div>
          <label class="small">Side crop %: <input id="cropPercent" type="range" min="0.2" max="0.6" step="0.05" value="0.4"></label>
          <div style="font-size:13px;color:var(--muted);margin-top:6px">Buffer size (frames): <input id="bufferSize" type="number" min="1" max="20" value="5" style="width:68px"></div>
          <div style="font-size:13px;color:var(--muted);margin-top:6px">Throttle ms: <input id="throttle" type="number" min="30" max="600" value="100" style="width:78px"></div>
        </div>
      </div>
    </div>

    <footer>All detection runs locally in your browser (TensorFlow.js). No frames leave your device.</footer>
  </div>

  <!-- TensorFlow + COCO-SSD -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.20.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.3"></script>

<script>
/* ---------- CONFIG ----------
   Test image path (developer provided local path)
--------------------------------*/
const TEST_IMAGE_PATH = '/mnt/data/E99A7BF9-F4D9-4F41-9C8A-0751B5C9F4E2.jpeg';

/* ---------- DOM ----------
*/
const cam = document.getElementById('cam');
const boxesCanvas = document.getElementById('boxes');
const hudCanvas = document.getElementById('hud');
const ctxBoxes = boxesCanvas.getContext('2d', { willReadFrequently: true });
const ctxHUD = hudCanvas.getContext('2d', { willReadFrequently: true });

const startBtn = document.getElementById('startBtn');
const stopBtn = document.getElementById('stopBtn');
const testMode = document.getElementById('testMode');
const camSelect = document.getElementById('camSelect');
const conf = document.getElementById('conf');
const confVal = document.getElementById('confVal');
const desc = document.getElementById('desc');
const hint = document.getElementById('hint');

const showLeft = document.getElementById('showLeft');
const showRight = document.getElementById('showRight');
const showBoth = document.getElementById('showBoth');
const clearBtn = document.getElementById('clear');

const cropPercentEl = document.getElementById('cropPercent');
const bufferSizeEl = document.getElementById('bufferSize');
const throttleEl = document.getElementById('throttle');

document.getElementById('imgPath').textContent = TEST_IMAGE_PATH;

/* ---------- STATE ----------
*/
let model = null;
let stream = null;
let running = false;
let raf = null;
let devices = [];
let currentDeviceId = null;
let lastFrameTime = 0;
let throttleMs = parseInt(throttleEl.value || 100);
let bufferSize = parseInt(bufferSizeEl.value || 5);
let cropPercent = parseFloat(cropPercentEl.value || 0.4);

// smoothing buffers for left/right (keep boolean per frame)
let leftBuffer = [];
let rightBuffer = [];

/* ---------- UTIL: pick best TF backend (webgpu -> webgl -> wasm) ----------
*/
async function pickBackend() {
  try {
    await tf.setBackend('webgpu');
    await tf.ready();
    console.log('TF backend set to', tf.getBackend());
    return;
  } catch (e) {
    console.warn('webgpu failed:', e.message || e);
  }
  try {
    await tf.setBackend('webgl');
    await tf.ready();
    console.log('TF backend set to', tf.getBackend());
    return;
  } catch (e) {
    console.warn('webgl failed:', e.message || e);
  }
  try {
    await tf.setBackend('wasm');
    await tf.ready();
    console.log('TF backend set to', tf.getBackend());
  } catch (e) {
    console.warn('wasm failed', e.message || e);
  }
}

/* ---------- CANVAS FIT ----------
*/
function fitCanvases() {
  const rect = document.getElementById('stack').getBoundingClientRect();
  [boxesCanvas, hudCanvas].forEach(c => {
    c.width = Math.floor(rect.width);
    c.height = Math.floor(rect.height);
    c.style.left = '0px'; c.style.top = '0px';
    c.style.width = rect.width + 'px'; c.style.height = rect.height + 'px';
  });
  drawHUD(false, false, [], []);
}
window.addEventListener('resize', fitCanvases);
setTimeout(fitCanvases, 200);

/* ---------- MODEL LOAD + WARMUP ----------
*/
async function loadModel() {
  if (model) return model;
  hint.style.display = 'block'; hint.textContent = 'Loading model (coco-ssd)...';
  await pickBackend();
  try {
    model = await cocoSsd.load({ base: 'mobilenet_v2' });
    // warmup with tiny canvas to reduce first-call latency
    const tmp = document.createElement('canvas');
    tmp.width = 320; tmp.height = 240;
    await model.detect(tmp);
    hint.textContent = '';
    console.log('Model loaded & warmed up');
    return model;
  } catch (e) {
    hint.textContent = 'Model load failed: ' + (e.message || e);
    console.error('Model load error', e);
    throw e;
  }
}

/* ---------- CAMERA HELPERS ----------
*/
async function listCameras(){
  try {
    const devs = await navigator.mediaDevices.enumerateDevices();
    devices = devs.filter(d => d.kind === 'videoinput');
    camSelect.innerHTML = devices.map((d,i)=>`<option value="${d.deviceId}">${d.label || 'Camera '+(i+1)}</option>`).join('');
    if (devices.length && !currentDeviceId) currentDeviceId = devices[0].deviceId;
  } catch (e) {
    console.warn('enumerateDevices failed', e);
  }
}

async function startCamera(deviceId = null) {
  try {
    if(location.protocol !== 'https:' && location.hostname !== 'localhost'){
      hint.style.display = 'block';
      hint.textContent = '⚠️ Camera requires HTTPS (or open via localhost).';
    } else {
      hint.style.display = 'none';
    }
    const constraints = deviceId ? { video: { deviceId: { exact: deviceId }, width:{ideal:1280}, height:{ideal:720} } }
                                 : { video: { facingMode: 'environment', width:{ideal:1280},height:{ideal:720} } };
    stream = await navigator.mediaDevices.getUserMedia(constraints);
    cam.srcObject = stream;
    await cam.play();
    await listCameras();
    fitCanvases();
    hint.style.display = 'block'; hint.textContent = 'Camera streaming';
  } catch (e) {
    console.error('startCamera error', e);
    hint.style.display = 'block';
    if (e.name === 'NotAllowedError') hint.textContent = 'Permission denied — allow camera in address bar.';
    else if (e.name === 'NotFoundError') hint.textContent = 'No camera found.';
    else hint.textContent = 'Camera error: ' + (e.message || e);
    throw e;
  }
}

function stopCamera() {
  if (stream) stream.getTracks().forEach(t => t.stop());
  stream = null;
  if (raf) cancelAnimationFrame(raf);
  running = false;
  hint.style.display = 'none';
}

/* ---------- DRAWING: HUD & Boxes ----------
*/
function roundRect(c,x,y,w,h,r,fill,stroke){
  if(r===undefined) r=6;
  c.beginPath();
  c.moveTo(x+r,y);
  c.arcTo(x+w,y,x+w,y+h,r);
  c.arcTo(x+w,y+h,x,y+h,r);
  c.arcTo(x,y+h,x,y,r);
  c.arcTo(x,y,x+w,y,r);
  c.closePath();
  if(fill) c.fill(); if(stroke) c.stroke();
}

function drawHUD(left=false, right=false, leftTypes=[], rightTypes[]){
  const W = hudCanvas.width, H = hudCanvas.height;
  ctxHUD.clearRect(0,0,W,H);
  // background (light grey area)
  ctxHUD.fillStyle = '#f5f6f8';
  ctxHUD.fillRect(0,0,W,H);

  const cx = W/2, cy = H*0.72;
  // road shapes
  ctxHUD.fillStyle = 'rgba(220,220,225,0.95)';
  ctxHUD.beginPath();
  ctxHUD.ellipse(cx - W*0.35, cy-40, W*0.24, H*0.34, 0, 0, Math.PI*2);
  ctxHUD.ellipse(cx + W*0.35, cy-40, W*0.24, H*0.34, 0, 0, Math.PI*2);
  ctxHUD.ellipse(cx, cy - 80, W*0.22, H*0.38, 0, 0, Math.PI*2);
  ctxHUD.fill();

  // car body
  const carW = W*0.22, carH = H*0.26;
  ctxHUD.save();
  ctxHUD.translate(cx, cy - 40);
  ctxHUD.shadowColor = 'rgba(0,0,0,0.18)';
  ctxHUD.shadowBlur = 18;
  ctxHUD.shadowOffsetY = 8;
  ctxHUD.fillStyle = '#ffffff'; roundRect(ctxHUD, -carW/2, -carH/2, carW, carH, 18, true, false);
  ctxHUD.fillStyle = '#10141a'; roundRect(ctxHUD, -carW*0.33, -carH*0.33, carW*0.66, carH*0.66, 10, true, false);
  ctxHUD.restore();

  if (left) drawSideBubble('left', leftTypes[0] || 'Side Object');
  if (right) drawSideBubble('right', rightTypes[0] || 'Side Object');
}

function drawSideBubble(side, label){
  const W = hudCanvas.width, H = hudCanvas.height;
  const cx = W/2, cy = H*0.72;
  const sideX = (side === 'left') ? cx - W*0.35 : cx + W*0.35;
  const py = cy - 40 - H*0.06;
  const color = (side === 'left') ? 'rgba(120,240,160,0.96)' : 'rgba(255,195,70,0.95)';
  const rad = Math.min(W,H) * 0.22;
  const g = ctxHUD.createRadialGradient(sideX, py, rad*0.05, sideX, py, rad);
  g.addColorStop(0, color.replace(/\)$/,'1)'));
  g.addColorStop(1, color.replace(/\)$/,'0)'));
  ctxHUD.fillStyle = g;
  ctxHUD.beginPath(); ctxHUD.arc(sideX, py, rad, 0, Math.PI*2); ctxHUD.fill();
  ctxHUD.strokeStyle = 'rgba(255,255,255,0.06)';
  ctxHUD.lineWidth = 2; ctxHUD.beginPath(); ctxHUD.arc(sideX, py, rad, 0, Math.PI*2); ctxHUD.stroke();

  // label bubble
  ctxHUD.fillStyle = 'rgba(0,0,0,0.65)';
  ctxHUD.font = '14px sans-serif';
  const tw = ctxHUD.measureText(label).width;
  const lx = sideX - tw/2 - 10;
  const ly = py - rad - 28;
  roundRect(ctxHUD, lx, ly, tw + 20, 26, 8, true, false);
  ctxHUD.fillStyle = '#fff';
  ctxHUD.fillText(label, lx + 10, ly + 18);
}

function drawBoxes(preds, threshold, srcW, srcH){
  const W = boxesCanvas.width, H = boxesCanvas.height;
  ctxBoxes.clearRect(0,0,W,H);
  ctxBoxes.font = '12px monospace';
  const scaleX = W / srcW, scaleY = H / srcH;
  for(const p of preds){
    const [x,y,w,h] = p.bbox;
    const sx = x*scaleX, sy = y*scaleY, sw = w*scaleX, sh = h*scaleY;
    ctxBoxes.strokeStyle = p.score >= threshold ? 'rgba(30,144,255,0.95)' : 'rgba(150,150,150,0.45)';
    ctxBoxes.lineWidth = 2; ctxBoxes.strokeRect(sx, sy, sw, sh);
    const label = `${p.class} ${(p.score*100).toFixed(0)}%`;
    ctxBoxes.fillStyle = 'rgba(0,0,0,0.6)';
    const lw = ctxBoxes.measureText(label).width + 8;
    ctxBoxes.fillRect(sx, Math.max(0, sy - 20), lw, 18);
    ctxBoxes.fillStyle = '#fff'; ctxBoxes.fillText(label, sx + 4, Math.max(0, sy - 6));
  }
}

/* ---------- CROPPING + DETECTION ----------
   Approach:
   - For better side sensitivity we crop left and right slices and run detect on them (smaller canvas -> faster & more focused).
   - We also run a full-frame detect occasionally to keep context.
*/
async function detectFrame() {
  if (!running) return;
  if (!model) {
    try { await loadModel(); } catch(e){ raf = requestAnimationFrame(detectFrame); return; }
  }
  const now = performance.now();
  if (now - lastFrameTime < throttleMs) { raf = requestAnimationFrame(detectFrame); return; }
  lastFrameTime = now;

  try {
    let predsFull = [];
    let srcW = 640, srcH = 480;

    if (testMode.checked) {
      // test image path
      const img = new Image();
      img.src = TEST_IMAGE_PATH;
      await img.decode();
      const tmp = document.createElement('canvas');
      const scale = Math.min(1, 640 / img.naturalWidth);
      tmp.width = Math.floor(img.naturalWidth * scale);
      tmp.height = Math.floor(img.naturalHeight * scale);
      tmp.getContext('2d').drawImage(img, 0, 0, tmp.width, tmp.height);
      predsFull = await model.detect(tmp);
      srcW = tmp.width; srcH = tmp.height;
    } else {
      // live video
      if (!cam.videoWidth || cam.readyState < 2) { raf = requestAnimationFrame(detectFrame); return; }
      srcW = cam.videoWidth; srcH = cam.videoHeight;

      // crop sides
      const cropW = Math.floor(srcW * cropPercent);
      const tmp = document.createElement('canvas');
      tmp.width = cropW; tmp.height = srcH;
      const tctx = tmp.getContext('2d');

      // LEFT crop detection
      tctx.clearRect(0,0,tmp.width,tmp.height);
      tctx.drawImage(cam, 0, 0, cropW, srcH, 0, 0, cropW, srcH);
      const leftPreds = await model.detect(tmp);

      // RIGHT crop detection
      tctx.clearRect(0,0,tmp.width,tmp.height);
      tctx.drawImage(cam, srcW - cropW, 0, cropW, srcH, 0, 0, cropW, srcH);
      const rightPreds = await model.detect(tmp);

      // occasional full-frame detection (every Nth frame)
      if ((Math.floor(now / 1000) % 2) === 0) { // roughly every 1 second
        predsFull = await model.detect(cam);
      } else {
        predsFull = []; // keep empty to avoid work
      }

      // merge: mark preds with normalized center positions in full frame coordinates
      // leftPreds and rightPreds are relative to their crop; translate them
      const useful = [];
      const threshold = parseFloat(conf.value || 0.55);

      leftPreds.forEach(p => {
        if (p.score < threshold) return;
        // convert bbox x relative to full frame
        const [x,y,w,h] = p.bbox;
        const globalX = x; // left crop origin is 0
        useful.push({ class: p.class, score: p.score, bbox:[globalX, y, w, h] , source:'left', cropW });
      });
      rightPreds.forEach(p => {
        if (p.score < threshold) return;
        const [x,y,w,h] = p.bbox;
        const globalX = (srcW - cropW) + x;
        useful.push({ class: p.class, score: p.score, bbox:[globalX, y, w, h], source:'right', cropW });
      });

      // include full frame preds (already in full coords)
      predsFull.forEach(p => { if (p.score >= threshold) useful.push({ class:p.class, score:p.score, bbox:p.bbox, source:'full' }); });

      // final predictions = useful
      predsFull = useful;
    }

    // choose relevant classes
    const threshold = parseFloat(conf.value || 0.55);
    confVal.textContent = threshold.toFixed(2);

    // filter useful classes
    const useful = predsFull.filter(p => ['person','car','bus','truck','bicycle','motorbike'].includes(p.class) && p.score >= threshold);

    // decide left/right by bbox center vs srcW
    let leftFound = false, rightFound = false;
    let leftTypes = [], rightTypes = [];

    // if using live video, srcW was set above; if testMode, compute from tmp
    const effectiveSrcW = (cam.videoWidth && !testMode.checked) ? cam.videoWidth : (predsFull.length? (predsFull[0].bbox.length ? (predsFull[0].bbox[2] + predsFull[0].bbox[0]) : 640) : 640);

    useful.forEach(p => {
      const [x,y,w,h] = p.bbox;
      const cx = x + w/2;
      const nx = cx / (effectiveSrcW || 640);
      if (nx < 0.46) { leftFound = true; leftTypes.push(p.class); }
      else if (nx > 0.54) { rightFound = true; rightTypes.push(p.class); }
      else { leftFound = true; rightFound = true; leftTypes.push(p.class); rightTypes.push(p.class); }
    });

    // smoothing: push boolean into buffer and decide if object present by majority
    leftBuffer.push(leftFound ? 1 : 0); rightBuffer.push(rightFound ? 1 : 0);
    if (leftBuffer.length > bufferSize) leftBuffer.shift();
    if (rightBuffer.length > bufferSize) rightBuffer.shift();

    const leftVotes = leftBuffer.reduce((a,b)=>a+b,0);
    const rightVotes = rightBuffer.reduce((a,b)=>a+b,0);
    const leftStable = leftVotes >= Math.ceil(bufferSize/2);
    const rightStable = rightVotes >= Math.ceil(bufferSize/2);

    // draw HUD & boxes (use cam width/height or fallback)
    const srcWdraw = cam.videoWidth || 640, srcHdraw = cam.videoHeight || 480;
    drawHUD(leftStable, rightStable, leftTypes, rightTypes);
    drawBoxes(predsFull, threshold, srcWdraw, srcHdraw);

    if (leftStable || rightStable) {
      const parts = [];
      if (leftStable) parts.push('Left: ' + (leftTypes.join(', ') || 'object'));
      if (rightStable) parts.push('Right: ' + (rightTypes.join(', ') || 'object'));
      desc.textContent = 'Detected: ' + parts.join(' · ');
      hint.style.display = 'block'; hint.textContent = 'Object detected';
    } else {
      desc.textContent = 'No side objects detected.';
      hint.style.display = 'none';
    }

  } catch (e) {
    console.error('detection error', e);
    hint.style.display = 'block'; hint.textContent = 'Detection error: ' + (e.message || e);
  } finally {
    raf = requestAnimationFrame(detectFrame);
  }
}

/* ---------- UI wiring ----------
*/
startBtn.onclick = async () => {
  try {
    await listCameras();
    await loadModel(); // start model load early
    if (!testMode.checked) {
      try { await startCamera(camSelect.value || currentDeviceId || null); } catch (e) { await startCamera(null); }
    }
    // reset
    leftBuffer = []; rightBuffer = [];
    bufferSize = parseInt(bufferSizeEl.value || 5);
    throttleMs = parseInt(throttleEl.value || 100);
    cropPercent = parseFloat(cropPercentEl.value || 0.4);

    running = true;
    lastFrameTime = performance.now();
    raf = requestAnimationFrame(detectFrame);
  } catch (e) {
    console.error('Start error', e);
  }
};
stopBtn.onclick = () => stopCamera();

camSelect.onchange = (e) => { currentDeviceId = e.target.value; };

showLeft.onclick = () => { drawHUD(true,false,['person'],[]); desc.textContent = 'Detected: Left (debug)'; };
showRight.onclick = () => { drawHUD(false,true,[],['person']); desc.textContent = 'Detected: Right (debug)'; };
showBoth.onclick = () => { drawHUD(true,true,['car'],['car']); desc.textContent = 'Detected: Both (debug)'; };
clearBtn.onclick = () => { drawHUD(false,false); desc.textContent = 'No side objects detected.'; leftBuffer=[]; rightBuffer=[]; };

conf.oninput = ()=> confVal.textContent = parseFloat(conf.value).toFixed(2);
cropPercentEl.oninput = ()=> cropPercent = parseFloat(cropPercentEl.value);
bufferSizeEl.oninput = ()=> bufferSize = parseInt(bufferSizeEl.value);
throttleEl.oninput = ()=> throttleMs = parseInt(throttleEl.value);

/* ---------- INIT ----------
*/
(async function init(){
  try{ await tf.ready(); console.log('TF ready backend:', tf.getBackend()); }catch(e){ console.warn('tf.ready failed', e); }
  await listCameras();
  // preload model in background
  loadModel().catch(e=>console.warn('preload failed', e));
  fitCanvases();
  drawHUD(false,false,[],[]);
})();
</script>
</body>
</html>